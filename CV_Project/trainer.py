# trainer.py

import os
import time
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from tqdm import tqdm
import numpy as np

from utils import save_checkpoint, plot_and_save_history # å¯¼å…¥æ–°çš„ç»˜å›¾å‡½æ•°
from evaluate import evaluate_segmentation

class Trainer:
    def __init__(self,
                 model: nn.Module,
                 train_loader: DataLoader,
                 val_loader: DataLoader,
                 criterion: nn.Module,
                 optimizer: optim.Optimizer,
                 scheduler: optim.lr_scheduler._LRScheduler = None,
                 device: torch.device = None,
                 num_epochs: int = 100,
                 num_classes_eval: int = -1,
                 ignore_index_eval: int = -1,
                 experiment_dir: str = "experiment_outputs",
                 checkpoint_prefix: str = "model_ckpt",
                 start_epoch: int = 0,
                 best_metric_val: float = float('-inf'),
                 metric_to_optimize: str = 'mean_iou_adjusted', # ç”¨äºŽåˆ¤æ–­æœ€ä½³æ¨¡åž‹å’Œç»˜å›¾çš„æŒ‡æ ‡
                 print_freq: int = 10
                 # ç§»é™¤äº† save_summary_writerï¼Œå› ä¸ºä¸å†ä½¿ç”¨TensorBoard
                 ):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.criterion = criterion
        self.optimizer = optimizer
        self.scheduler = scheduler

        if device is None: self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else: self.device = device
        self.model.to(self.device)

        self.num_epochs = num_epochs
        self.num_classes_eval = num_classes_eval
        self.ignore_index_eval = ignore_index_eval
        self.experiment_dir = experiment_dir
        self.checkpoint_prefix = checkpoint_prefix
        self.start_epoch = start_epoch
        self.current_epoch = start_epoch
        self.best_metric_val = best_metric_val
        self.metric_to_optimize = metric_to_optimize
        self.print_freq = print_freq

        self.training_log_file = os.path.join(self.experiment_dir, "training_log.txt")
        
        # æ–°å¢žï¼šç”¨äºŽå­˜å‚¨è®­ç»ƒåŽ†å²çš„åˆ—è¡¨
        self.history = {
            'train_loss': [],
            'val_loss': [],
            'val_metric': [] # å°†å­˜å‚¨ metric_to_optimize çš„å€¼
        }
        # å¦‚æžœä»Žæ£€æŸ¥ç‚¹æ¢å¤ï¼Œç†è®ºä¸Šåº”è¯¥ä¹Ÿæ¢å¤ historyï¼Œä½†ä¸ºäº†ç®€åŒ–ï¼Œè¿™é‡Œä»Žæ–°å¼€å§‹è®°å½•
        # æˆ–è€…åœ¨ load_checkpoint ä¸­ä¹ŸåŠ è½½ history (å¦‚æžœä¿å­˜åœ¨æ£€æŸ¥ç‚¹é‡Œ)

        # åˆå§‹åŒ–æ—¥å¿—ï¼ˆåªæ‰“å°ä¸€æ¬¡ï¼‰
        self._log_message(f"Trainer åˆå§‹åŒ–å®Œæˆ:", console_too=False)
        self._log_message(f"  è®¾å¤‡: {self.device}", console_too=False)
        # ... (å…¶ä»–åˆå§‹åŒ–æ—¥å¿—å¯ä»¥ä¿ç•™)

    def _log_message(self, message: str, console_too: bool = True):
        if console_too: print(message)
        try:
            with open(self.training_log_file, 'a') as f:
                f.write(f"[{time.strftime('%Y-%m-%d %H:%M:%S')}] {message}\n")
        except Exception as e: print(f"å†™å…¥æ—¥å¿—æ–‡ä»¶å¤±è´¥: {e}")

    def train_one_epoch(self):
        self.model.train()
        total_loss = 0.0; num_samples = 0
        progress_bar = tqdm(self.train_loader, desc=f"Epoch {self.current_epoch + 1}/{self.num_epochs} [è®­ç»ƒä¸­]", leave=False)
        for batch_idx, (images, targets) in enumerate(progress_bar):
            images, targets = images.to(self.device), targets.to(self.device)
            self.optimizer.zero_grad(); outputs = self.model(images)
            loss = self.criterion(outputs, targets); loss.backward(); self.optimizer.step()
            total_loss += loss.item() * images.size(0); num_samples += images.size(0)
            if (batch_idx + 1) % self.print_freq == 0 or (batch_idx + 1) == len(self.train_loader):
                current_lr = self.optimizer.param_groups[0]['lr']
                progress_bar.set_postfix({'Loss': f"{loss.item():.4f}", 'AvgLoss': f"{total_loss/num_samples:.4f}", 'LR': f"{current_lr:.1e}"})
        avg_train_loss = total_loss / num_samples
        progress_bar.close()
        self.history['train_loss'].append(avg_train_loss) # è®°å½•è®­ç»ƒæŸå¤±
        log_msg = f"Epoch {self.current_epoch + 1}/{self.num_epochs} [è®­ç»ƒç»“æŸ] - å¹³å‡è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}"
        self._log_message(log_msg)
        return avg_train_loss

    def validate(self):
        self._log_message(f"Epoch {self.current_epoch + 1}/{self.num_epochs} [éªŒè¯ä¸­]...")
        avg_val_loss, metrics = evaluate_segmentation(
            self.model, self.val_loader, self.criterion, self.device,
            num_classes=self.num_classes_eval, ignore_index=self.ignore_index_eval
        )
        self.history['val_loss'].append(avg_val_loss) # è®°å½•éªŒè¯æŸå¤±
        
        # è®°å½•è¦ä¼˜åŒ–çš„é‚£ä¸ªæŒ‡æ ‡
        metric_val_to_record = metrics.get(self.metric_to_optimize)
        if metric_val_to_record is None: # å¦‚æžœæŒ‡å®šçš„metric_to_optimizeæ‰¾ä¸åˆ°ï¼Œåˆ™è®°å½•mIoU
            metric_val_to_record = metrics.get('mean_iou', metrics.get('mean_iou_adjusted', 0.0))
        self.history['val_metric'].append(metric_val_to_record)

        self._log_message(f"Epoch {self.current_epoch + 1}/{self.num_epochs} [éªŒè¯ç»“æŸ] - å¹³å‡éªŒè¯æŸå¤±: {avg_val_loss:.4f}")
        self._log_message(f"  éªŒè¯æŒ‡æ ‡:")
        for k, v in metrics.items():
            if isinstance(v, (float, np.float32, np.float64)):
                 self._log_message(f"    {k}: {v:.4f}", console_too=False)
            elif isinstance(v, np.ndarray) and k.endswith('_per_class'):
                self._log_message(f"    {k}: {np.round(v, 4).tolist()}", console_too=False)
        return avg_val_loss, metrics

    def run_training(self):
        self._log_message(f"\n{'='*20} å¼€å§‹è®­ç»ƒ {'='*20}", console_too=False)
        self._log_message(f"æ¨¡åž‹å°†è®­ç»ƒ {self.num_epochs - self.start_epoch} ä¸ªè½®æ¬¡ (ä»Ž epoch {self.start_epoch+1} åˆ° {self.num_epochs})", console_too=False)

        for epoch in range(self.start_epoch, self.num_epochs):
            self.current_epoch = epoch; epoch_start_time = time.time()
            _ = self.train_one_epoch()
            avg_val_loss, val_metrics = self.validate()

            if self.scheduler:
                if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):
                    metric_for_scheduler = avg_val_loss if self.metric_to_optimize.lower().endswith('loss') \
                                           else val_metrics.get(self.metric_to_optimize, float('-inf'))
                    self.scheduler.step(metric_for_scheduler)
                else: self.scheduler.step()

            current_metric_val = val_metrics.get(self.metric_to_optimize)
            if current_metric_val is None:
                self._log_message(f"è­¦å‘Š: æ— æ³•åœ¨éªŒè¯æŒ‡æ ‡ä¸­æ‰¾åˆ° '{self.metric_to_optimize}'ã€‚ä½¿ç”¨éªŒè¯æŸå¤±çš„è´Ÿå€¼ã€‚")
                current_metric_val = -avg_val_loss; is_best = current_metric_val > self.best_metric_val
            else: is_best = current_metric_val > self.best_metric_val

            if is_best:
                self.best_metric_val = current_metric_val
                self._log_message(f"  ðŸŽ‰ æ–°çš„æœ€ä½³æ¨¡åž‹! {self.metric_to_optimize}: {self.best_metric_val:.4f} at epoch {self.current_epoch + 1}")
            
            checkpoint_state = {
                'epoch': self.current_epoch + 1, 'model_state_dict': self.model.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'best_metric_val': self.best_metric_val, 'metric_name': self.metric_to_optimize
            }
            if self.scheduler: checkpoint_state['scheduler_state_dict'] = self.scheduler.state_dict()
            save_checkpoint(checkpoint_state, is_best, self.experiment_dir, self.checkpoint_prefix)
            self._log_message(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜ (Epoch {self.current_epoch + 1}, IsBest: {is_best})", console_too=False)

            epoch_duration = time.time() - epoch_start_time
            self._log_message(f"Epoch {self.current_epoch + 1} å®Œæˆï¼Œç”¨æ—¶: {epoch_duration:.2f} ç§’")
            self._log_message(f"  å½“å‰å­¦ä¹ çŽ‡: {self.optimizer.param_groups[0]['lr']:.1e}")
            self._log_message(f"  å½“å‰æœ€ä½³ {self.metric_to_optimize}: {self.best_metric_val:.4f}")
            self._log_message("-" * 50, console_too=False)

        self._log_message(f"\n{'='*20} è®­ç»ƒå®Œæˆ {'='*20}")
        self._log_message(f"æœ€ä½³ {self.metric_to_optimize} åœ¨éªŒè¯é›†ä¸Šè¾¾åˆ°: {self.best_metric_val:.4f}")
        
        # è®­ç»ƒç»“æŸåŽç»˜åˆ¶å¹¶ä¿å­˜åŽ†å²å›¾è¡¨
        plot_save_path_prefix = os.path.join(self.experiment_dir, f"{self.checkpoint_prefix}_training_history")
        primary_metric_plot_name = self.metric_to_optimize.replace('_adjusted','').replace('_',' ').title() # "mean_iou_adjusted" -> "Mean IoU"
        plot_and_save_history(self.history, plot_save_path_prefix, primary_metric_plot_name)